import requests
import json
import re
import sqlite3
import hashlib
from datetime import datetime
import time
import os
from bs4 import BeautifulSoup

class CCTVNewsCrawler:
    def __init__(self, db_path='cctv_news.db', content_base_dir='news'):
        self.db_path = db_path
        self.content_base_dir = content_base_dir
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“ç»“æ„"""
        conn = sqlite3.connect(self.db_path)
        c = conn.cursor()
        
        c.execute('''
            CREATE TABLE IF NOT EXISTS news (
                content_hash TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                summary TEXT NOT NULL,
                publish_time TEXT,
                keywords TEXT,
                crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                content_file_path TEXT
            )
        ''')
        
        c.execute('CREATE INDEX IF NOT EXISTS idx_publish_time ON news(publish_time)')
        c.execute('CREATE INDEX IF NOT EXISTS idx_crawl_time ON news(crawl_time)')
        
        conn.commit()
        conn.close()
    
    def get_content_hash(self, news_item):
        """åŸºäºæ ‡é¢˜å’Œæ‘˜è¦ç”Ÿæˆå†…å®¹å“ˆå¸Œ"""
        content = f"{news_item['title']}{news_item['brief']}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def fetch_news_page(self, page_num):
        """è·å–æ–°é—»åˆ—è¡¨é¡µæ•°æ®"""
        api_url = f"https://news.cctv.com/2019/07/gaiban/cmsdatainterface/page/world_{page_num}.jsonp"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://news.cctv.com/world/',
            'Accept': '*/*'
        }
        
        try:
            print(f"æ­£åœ¨è·å–ç¬¬ {page_num} é¡µæ•°æ®...")
            response = requests.get(api_url, headers=headers, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                news_list = self.parse_jsonp(response.text)[:10]
                print(f"ç¬¬ {page_num} é¡µè·å–åˆ° {len(news_list)} æ¡æ–°é—»")
                return news_list
            else:
                print(f"ç¬¬ {page_num} é¡µè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
                
        except Exception as e:
            print(f"è·å–ç¬¬ {page_num} é¡µæ•°æ®å¤±è´¥: {e}")
            return []
    
    def parse_jsonp(self, jsonp_data):
        """è§£æJSONPæ•°æ®"""
        try:
            json_str = re.search(r'\{.*\}', jsonp_data).group()
            data = json.loads(json_str)
            return data['data']['list']
        except Exception as e:
            print(f"è§£æJSONPæ•°æ®å¤±è´¥: {e}")
            return []
    
    def fetch_detailed_content(self, url):
        """è·å–æ–°é—»è¯¦ç»†å†…å®¹"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        }
        
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾å†…å®¹åŒºåŸŸ
                content_area = soup.find('div', id='content_area')
                if content_area:
                    # æå–çº¯æ–‡æœ¬å†…å®¹ï¼Œä¿ç•™æ®µè½ç»“æ„
                    content_text = ""
                    for element in content_area.find_all(['p', 'div']):
                        text = element.get_text(strip=True)
                        if text:
                            content_text += text + "\n"
                    
                    return content_text.strip()
                else:
                    print(f"æœªæ‰¾åˆ°å†…å®¹åŒºåŸŸ: {url}")
                    return None
            else:
                print(f"è·å–è¯¦ç»†å†…å®¹å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, URL: {url}")
                return None
                
        except Exception as e:
            print(f"è·å–è¯¦ç»†å†…å®¹å¼‚å¸¸: {e}, URL: {url}")
            return None
    
    def get_content_file_path(self, content_hash, publish_time):
        """æ ¹æ®å‘å¸ƒæ—¶é—´ç”Ÿæˆæ–‡ä»¶è·¯å¾„"""
        try:
            # è§£æå‘å¸ƒæ—¶é—´
            dt = datetime.strptime(publish_time, '%Y-%m-%d %H:%M:%S')
            year = dt.strftime('%Y')
            month = dt.strftime('%m')
            day = dt.strftime('%d')
            
            # åˆ›å»ºç›®å½•ç»“æ„
            dir_path = os.path.join(self.content_base_dir, year, month, day)
            os.makedirs(dir_path, exist_ok=True)
            
            # è¿”å›ç›¸å¯¹è·¯å¾„
            return os.path.join(year, month, day, f"{content_hash}.txt")
            
        except:
            # å¦‚æœæ—¶é—´è§£æå¤±è´¥ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
            today = datetime.now()
            year = today.strftime('%Y')
            month = today.strftime('%m')
            day = today.strftime('%d')
            
            dir_path = os.path.join(self.content_base_dir, year, month, day)
            os.makedirs(dir_path, exist_ok=True)
            
            return os.path.join(year, month, day, f"{content_hash}.txt")
    
    def save_detailed_content(self, file_path, detailed_content):
        """ä¿å­˜è¯¦ç»†å†…å®¹åˆ°txtæ–‡ä»¶"""
        if not detailed_content:
            return False
        
        full_path = os.path.join(self.content_base_dir, file_path)
        
        try:
            with open(full_path, 'w', encoding='utf-8') as f:
                f.write(detailed_content)
            return True
        except Exception as e:
            print(f"ä¿å­˜è¯¦ç»†å†…å®¹å¤±è´¥: {e}")
            return False
    
    def save_news_to_db(self, news_list, fetch_detailed=True):
        """ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        c = conn.cursor()
        
        new_count = 0
        duplicate_count = 0
        
        for news_item in news_list:
            content_hash = self.get_content_hash(news_item)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            c.execute('SELECT content_file_path FROM news WHERE content_hash = ?', (content_hash,))
            existing = c.fetchone()
            
            content_file_path = None
            
            # å¦‚æœéœ€è¦è·å–è¯¦ç»†å†…å®¹ä¸”å°šæœªè·å–
            if fetch_detailed and (not existing or not existing[0]):
                print(f"è·å–è¯¦ç»†å†…å®¹: {news_item['title'][:30]}...")
                detailed_content = self.fetch_detailed_content(news_item['url'])
                if detailed_content:
                    content_file_path = self.get_content_file_path(content_hash, news_item['focus_date'])
                    if not self.save_detailed_content(content_file_path, detailed_content):
                        content_file_path = None
                time.sleep(1)  # è¯·æ±‚é—´éš”
            elif existing:
                content_file_path = existing[0]
            
            try:
                if existing:
                    # æ›´æ–°è®°å½•ï¼ˆå¦‚æœä¹‹å‰æ²¡æœ‰è¯¦ç»†å†…å®¹ï¼Œç°åœ¨æœ‰äº†ï¼‰
                    if content_file_path and not existing[0]:
                        c.execute('''
                            UPDATE news 
                            SET content_file_path = ?, crawl_time = CURRENT_TIMESTAMP
                            WHERE content_hash = ?
                        ''', (content_file_path, content_hash))
                        new_count += 1  # ç®—ä½œæ–°å¢è¯¦ç»†å†…å®¹
                    else:
                        duplicate_count += 1
                else:
                    # æ’å…¥æ–°è®°å½•
                    c.execute('''
                        INSERT INTO news 
                        (content_hash, title, summary, publish_time, keywords, content_file_path)
                        VALUES (?, ?, ?, ?, ?, ?)
                    ''', (
                        content_hash,
                        news_item['title'],
                        news_item['brief'],
                        news_item['focus_date'],
                        news_item.get('keywords', ''),
                        content_file_path
                    ))
                    new_count += 1
                
            except Exception as e:
                print(f"ä¿å­˜æ–°é—»å¤±è´¥: {news_item['title']}, é”™è¯¯: {e}")
        
        conn.commit()
        conn.close()
        
        return new_count, duplicate_count
    
    def get_statistics(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        c = conn.cursor()
        
        stats = {}
        
        c.execute('SELECT COUNT(*) FROM news')
        stats['total_count'] = c.fetchone()[0]
        
        c.execute('SELECT COUNT(*) FROM news WHERE content_file_path IS NOT NULL')
        stats['detailed_count'] = c.fetchone()[0]
        
        c.execute('SELECT MIN(publish_time), MAX(publish_time) FROM news')
        stats['time_range'] = c.fetchone()
        
        conn.close()
        return stats
    
    def run_crawler(self, fetch_detailed=True, max_pages=3):
        """è¿è¡Œçˆ¬è™«"""
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - å¼€å§‹çˆ¬å–æ–°é—»...")
        
        all_news = []
        for page_num in range(1, max_pages + 1):
            page_news = self.fetch_news_page(page_num)
            if page_news:
                all_news.extend(page_news)
            time.sleep(0.5)
        
        if not all_news:
            print("æœªè·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
            return
        
        print(f"è·å–åˆ° {len(all_news)} æ¡æ–°é—»ï¼Œå¼€å§‹å¤„ç†è¯¦ç»†å†…å®¹..." if fetch_detailed else f"è·å–åˆ° {len(all_news)} æ¡æ–°é—»")
        
        new_count, duplicate_count = self.save_news_to_db(all_news, fetch_detailed)
        
        # æ˜¾ç¤ºç»Ÿè®¡
        stats = self.get_statistics()
        print(f"\nğŸ“Š çˆ¬å–å®Œæˆ:")
        print(f"   æ–°å¢/æ›´æ–°: {new_count} æ¡")
        print(f"   é‡å¤è·³è¿‡: {duplicate_count} æ¡")
        print(f"   æ•°æ®åº“æ€»æ•°: {stats['total_count']} æ¡")
        print(f"   å«è¯¦ç»†å†…å®¹: {stats['detailed_count']} æ¡")
        print(f"   æ—¶é—´èŒƒå›´: {stats['time_range'][0]} åˆ° {stats['time_range'][1]}")

def main():
    """çˆ¬è™«ä¸»å‡½æ•°"""
    crawler = CCTVNewsCrawler()
    crawler.run_crawler(fetch_detailed=True, max_pages=7)

if __name__ == "__main__":
    main()
